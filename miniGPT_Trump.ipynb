{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngdykR0h6Esk"
      },
      "source": [
        "**Projet GENERATION : Base de données de tweets de Trump** :"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ce notebook regroupe plusieurs étapes clés :\n",
        "\n",
        "1. **Téléchargement et prétraitement des tweets**  \n",
        "   - Récupération des données à partir d'un fichier CSV.\n",
        "   - Nettoyage des tweets (normalisation Unicode, suppression des URLs, etc.) et ajout de tokens spéciaux.\n",
        "\n",
        "2. **Entraînement du tokenizer SentencePiece (BPE)**  \n",
        "   - Création d'un modèle BPE sur le corpus de tweets nettoyé.\n",
        "\n",
        "3. **Encodage du corpus et préparation des données**  \n",
        "   - Transformation du texte en séquences d'IDs.\n",
        "   - Découpage du corpus en chunks pour l'entraînement.\n",
        "\n",
        "4. **Construction et entraînement du modèle Transformer**  \n",
        "   - Définition d'une architecture Transformer avec masque causal.\n",
        "   - Entraînement du modèle sur les données préparées.\n",
        "\n",
        "5. **Génération de texte et évaluation BLEU**  \n",
        "   - Génération de tweets via différentes méthodes (sampling, beam search, etc.).\n",
        "   - Évaluation de la qualité de la génération à l'aide du score BLEU.\n"
      ],
      "metadata": {
        "id": "VtJH60_qUohh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Téléchargement et Prétraitement des Tweets\n",
        "\n",
        "Nous téléchargeons le CSV depuis Google Drive, lisons le contenu, nettoyons les tweets et ajoutons des tokens spéciaux."
      ],
      "metadata": {
        "id": "f-KpIXjPVEZg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import re\n",
        "import csv\n",
        "import numpy as np\n",
        "import unicodedata  # Pour la normalisation Unicode\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import sentencepiece as spm\n",
        "\n",
        "# Téléchargement du CSV depuis Google Drive\n",
        "csv_url = \"https://drive.google.com/uc?export=download&id=1s1isv9TQjGiEr2gG__8bOdBFvQlmepRt\"\n",
        "path_to_file = keras.utils.get_file(\"realdonaltrump.csv\", csv_url)\n",
        "print(\"CSV file downloaded to:\", path_to_file)\n",
        "\n",
        "# %%\n",
        "# Lecture du CSV\n",
        "raw_tweets = []\n",
        "with open(path_to_file, newline='', encoding='utf-8') as csvfile:\n",
        "    reader = csv.DictReader(csvfile)\n",
        "    for row in reader:\n",
        "        raw_tweets.append(row['content'])\n",
        "\n",
        "print(\"Number of tweets loaded:\", len(raw_tweets))\n",
        "print(\"First 5 tweets:\\n\", raw_tweets[:5], \"\\n\")\n",
        "\n",
        "# %%\n",
        "# Prétraitement et nettoyage des tweets\n",
        "BOS_TOKEN = \"[BOS]\"\n",
        "EOS_TOKEN = \"[EOS]\"\n",
        "PAD_TOKEN = \"[PAD]\"\n",
        "\n",
        "def clean_tweet(tweet):\n",
        "    \"\"\"\n",
        "    Nettoie un tweet en :\n",
        "      - Normalisant les caractères Unicode.\n",
        "      - Ignorant les tweets avec des URLs de médias.\n",
        "      - Retirant les URLs et les espaces superflus.\n",
        "      - Retirant '#' des hashtags (conservant le texte).\n",
        "      - Préservant les tokens spéciaux ([BOS], [EOS], [PAD]).\n",
        "      - Normalisant les espaces.\n",
        "    \"\"\"\n",
        "    tweet = unicodedata.normalize(\"NFKC\", tweet)\n",
        "    if \"pic.twitter.com\" in tweet:\n",
        "        return None\n",
        "    tweet = re.sub(r\"http\\S+\", \"\", tweet)\n",
        "    tweet = re.sub(r\"@\\s+(\\w+)\", r\"@\\1\", tweet)\n",
        "    tweet = tweet.replace(\"#\", \"\")\n",
        "    for token in [BOS_TOKEN, EOS_TOKEN, PAD_TOKEN]:\n",
        "        tweet = tweet.replace(token, f\" {token} \")\n",
        "    tweet = re.sub(r\"[^a-zA-Z0-9@#\\[\\]\\s\\.\\,\\!\\?\\(\\)]\", \"\", tweet)\n",
        "    tweet = re.sub(r\"\\s+\", \" \", tweet).strip()\n",
        "    return tweet\n",
        "\n",
        "tweets_cleaned = [clean_tweet(tw) for tw in raw_tweets]\n",
        "tweets_cleaned = [tw for tw in tweets_cleaned if tw is not None]\n",
        "print(\"Number of cleaned tweets:\", len(tweets_cleaned))\n",
        "print(\"First 5 cleaned tweets:\\n\", tweets_cleaned[:5], \"\\n\")\n",
        "\n",
        "# %%\n",
        "# Écriture des tweets nettoyés dans un fichier texte\n",
        "text_file = \"tweets.txt\"\n",
        "with open(text_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    for tweet in tweets_cleaned:\n",
        "        tw_clean = tweet.replace(\"\\n\", \" \").replace(\"\\r\", \" \").strip()\n",
        "        f.write(BOS_TOKEN + tw_clean + EOS_TOKEN + \"\\n\")\n",
        "\n",
        "# %%\n",
        "# Concaténation des tweets pour l'entraînement de SentencePiece\n",
        "all_text = \"\"\n",
        "for tw in tweets_cleaned:\n",
        "    tw_clean = tw.replace(\"\\n\", \" \").replace(\"\\r\", \" \").strip()\n",
        "    all_text += f\"{BOS_TOKEN} {tw_clean} {EOS_TOKEN} \"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOHBD8AhUjPS",
        "outputId": "ca579444-9bc5-4937-9bab-466b7921c9c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://drive.google.com/uc?export=download&id=1s1isv9TQjGiEr2gG__8bOdBFvQlmepRt\n",
            "\u001b[1m11331653/11331653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "CSV file downloaded to: /root/.keras/datasets/realdonaltrump.csv\n",
            "Number of tweets loaded: 43352\n",
            "First 5 tweets:\n",
            " ['Be sure to tune in and watch Donald Trump on Late Night with David Letterman as he presents the Top Ten List tonight!', 'Donald Trump will be appearing on The View tomorrow morning to discuss Celebrity Apprentice and his new book Think Like A Champion!', 'Donald Trump reads Top Ten Financial Tips on Late Show with David Letterman: http://tinyurl.com/ooafwn - Very funny!', 'New Blog Post: Celebrity Apprentice Finale and Lessons Learned Along the Way: http://tinyurl.com/qlux5e', '\"My persona will never be that of a wallflower - I’d rather build walls than cling to them\" --Donald J. Trump'] \n",
            "\n",
            "Number of cleaned tweets: 39667\n",
            "First 5 cleaned tweets:\n",
            " ['Be sure to tune in and watch Donald Trump on Late Night with David Letterman as he presents the Top Ten List tonight!', 'Donald Trump will be appearing on The View tomorrow morning to discuss Celebrity Apprentice and his new book Think Like A Champion!', 'Donald Trump reads Top Ten Financial Tips on Late Show with David Letterman Very funny!', 'New Blog Post Celebrity Apprentice Finale and Lessons Learned Along the Way', 'My persona will never be that of a wallflower Id rather build walls than cling to them Donald J. Trump'] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 2. Entraînement du Tokenizer SentencePiece (BPE)\n",
        "\n",
        " Nous entraînons le tokenizer sur le fichier `tweets.txt` si le modèle n'existe pas."
      ],
      "metadata": {
        "id": "4a-YjDw_VSlZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "model_prefix = \"subword_model\"\n",
        "vocab_size = 450  # À ajuster si besoin\n",
        "\n",
        "if not os.path.exists(model_prefix + \".model\"):\n",
        "    spm.SentencePieceTrainer.Train(\n",
        "        f\"--input={text_file} \"\n",
        "        f\"--model_prefix={model_prefix} \"\n",
        "        f\"--vocab_size={vocab_size} \"\n",
        "        f\"--character_coverage=1.0 \"\n",
        "        f\"--model_type=bpe \"\n",
        "        f\"--bos_id=-1 --eos_id=-1 --pad_id=-1 \"\n",
        "        f\"--user_defined_symbols={BOS_TOKEN},{EOS_TOKEN},{PAD_TOKEN}\"\n",
        "    )\n",
        "    print(\"SentencePiece model trained.\")\n",
        "else:\n",
        "    print(\"SentencePiece model found. Skipping training.\")\n",
        "\n",
        "# %%\n",
        "sp = spm.SentencePieceProcessor(model_file=model_prefix + \".model\")\n",
        "test_str = \"[BOS] Hello [EOS]\"\n",
        "test_ids = sp.encode_as_ids(test_str)\n",
        "print(\"Test enc ->\", test_str, \":\", test_ids)\n",
        "print(\"Test dec ->\", sp.decode_ids(test_ids), \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bA6UcrnoViUf",
        "outputId": "1fdc05f8-e98c-444d-e56c-3c3102cc04e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SentencePiece model trained.\n",
            "Test enc -> [BOS] Hello [EOS] : [378, 1, 308, 27, 382, 378, 2]\n",
            "Test dec -> [BOS] Hello [EOS] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 3. Encodage du Corpus et Préparation des Données\n",
        "\n",
        " Nous encodons le texte complet et découpçons le corpus en chunks de taille fixe."
      ],
      "metadata": {
        "id": "LROJnnKWVraN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "encoded_corpus = sp.encode_as_ids(all_text)\n",
        "print(\"Total length of encoded corpus:\", len(encoded_corpus))\n",
        "\n",
        "# %%\n",
        "max_len = 70  # Taille de chaque chunk\n",
        "pad_id = sp.piece_to_id(PAD_TOKEN)\n",
        "chunks = []\n",
        "start = 0\n",
        "while start < len(encoded_corpus):\n",
        "    end = start + max_len\n",
        "    chunk = encoded_corpus[start:end]\n",
        "    if len(chunk) < max_len:\n",
        "        chunk += [pad_id] * (max_len - len(chunk))\n",
        "    chunks.append(chunk)\n",
        "    start = end\n",
        "chunks = np.array(chunks, dtype=np.int32)\n",
        "print(\"Number of chunks:\", len(chunks), \"-> shape:\", chunks.shape)\n",
        "\n",
        "# %%\n",
        "# Création des paires (X, Y) pour le language modeling\n",
        "X = chunks[:, :-1]\n",
        "Y = chunks[:, 1:]\n",
        "print(\"X.shape =\", X.shape, \"Y.shape =\", Y.shape)\n",
        "\n",
        "# %%\n",
        "# Division en ensembles d'entraînement et de validation\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_val, y_train, y_val = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "print(\"Train shapes:\", x_train.shape, y_train.shape)\n",
        "print(\"Validation shapes:\", x_val.shape, y_val.shape)\n",
        "\n",
        "# %%\n",
        "# Vérification rapide du décodage\n",
        "print(\"Sample input:\", sp.decode_ids(X[1].tolist()))\n",
        "print(\"Sample target:\", sp.decode_ids(Y[1].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJ3UeGs7V2fy",
        "outputId": "f0006924-2e57-4510-d22e-c94c7f42304d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total length of encoded corpus: 2300936\n",
            "Number of chunks: 32871 -> shape: (32871, 70)\n",
            "X.shape = (32871, 69) Y.shape = (32871, 69)\n",
            "Train shapes: (26296, 69) (26296, 69)\n",
            "Validation shapes: (6575, 69) (6575, 69)\n",
            "Sample input: morning to discuss Celebrity Apprentice and his new book Think Like A Champion! [EOS] [BOS] Donald Trump reads Top Ten Financial Tips on Late Show with David Letterm\n",
            "Sample target: orning to discuss Celebrity Apprentice and his new book Think Like A Champion! [EOS] [BOS] Donald Trump reads Top Ten Financial Tips on Late Show with David Letterman\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Construction et Entraînement du Modèle Transformer\n",
        "\n",
        "Nous définissons et entraînons un modèle Transformer avec un masque causal."
      ],
      "metadata": {
        "id": "2NtN1Li9V4me"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.saving import register_keras_serializable\n",
        "\n",
        "@register_keras_serializable()\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.ffn = keras.Sequential([\n",
        "            layers.Dense(ff_dim, activation=\"relu\"),\n",
        "            layers.Dense(embed_dim),\n",
        "        ])\n",
        "        self.dropout1 = layers.Dropout(dropout_rate)\n",
        "        self.dropout2 = layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        seq_len = tf.shape(inputs)[1]\n",
        "        mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "        mask = tf.expand_dims(mask, axis=0)\n",
        "        attn_output = self.att(inputs, inputs, inputs, attention_mask=mask)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "# %%\n",
        "@register_keras_serializable()\n",
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim, dropout_rate=0.0):\n",
        "        super().__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "        self.dropout = layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        seq_length = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=seq_length, delta=1)\n",
        "        pos_emb = self.pos_emb(positions)\n",
        "        token_emb = self.token_emb(x)\n",
        "        out = token_emb + pos_emb\n",
        "        return self.dropout(out, training=training)\n",
        "\n",
        "# %%\n",
        "# Hyperparamètres\n",
        "embed_dim = 128\n",
        "num_heads = 4\n",
        "ff_dim = 512\n",
        "num_layers = 4\n",
        "dropout_rate = 0.1\n",
        "batch_size = 128\n",
        "epochs = 5\n",
        "max_input_len = max_len - 1\n",
        "\n",
        "# %%\n",
        "# Construction du modèle\n",
        "inputs = keras.Input(shape=(max_input_len,), dtype=tf.int32)\n",
        "x = TokenAndPositionEmbedding(max_input_len, sp.get_piece_size(), embed_dim, dropout_rate)(inputs)\n",
        "for _ in range(num_layers):\n",
        "    x = TransformerBlock(embed_dim, num_heads, ff_dim, dropout_rate)(x)\n",
        "outputs = layers.Dense(sp.get_piece_size(), activation=\"softmax\")(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "# %%\n",
        "# Entraînement\n",
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    validation_data=(x_val, y_val),\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    callbacks=[\n",
        "        keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True),\n",
        "        keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-6)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# %%\n",
        "# Sauvegarde du modèle entraîné\n",
        "model.save(\"transformer_model.keras\") # Change the file path to include the .keras extension"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 636
        },
        "id": "EM0I3-_NWKsI",
        "outputId": "977aca4b-d7d3-4e39-bf92-e0f4e476d45e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_4\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_4\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m69\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ token_and_position_embedding         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m69\u001b[0m, \u001b[38;5;34m128\u001b[0m)             │          \u001b[38;5;34m66,432\u001b[0m │\n",
              "│ (\u001b[38;5;33mTokenAndPositionEmbedding\u001b[0m)          │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ transformer_block (\u001b[38;5;33mTransformerBlock\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m69\u001b[0m, \u001b[38;5;34m128\u001b[0m)             │         \u001b[38;5;34m396,032\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ transformer_block_1                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m69\u001b[0m, \u001b[38;5;34m128\u001b[0m)             │         \u001b[38;5;34m396,032\u001b[0m │\n",
              "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)                   │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ transformer_block_2                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m69\u001b[0m, \u001b[38;5;34m128\u001b[0m)             │         \u001b[38;5;34m396,032\u001b[0m │\n",
              "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)                   │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ transformer_block_3                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m69\u001b[0m, \u001b[38;5;34m128\u001b[0m)             │         \u001b[38;5;34m396,032\u001b[0m │\n",
              "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)                   │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m69\u001b[0m, \u001b[38;5;34m450\u001b[0m)             │          \u001b[38;5;34m58,050\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">69</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ token_and_position_embedding         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">69</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">66,432</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TokenAndPositionEmbedding</span>)          │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ transformer_block (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">69</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">396,032</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ transformer_block_1                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">69</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">396,032</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)                   │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ transformer_block_2                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">69</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">396,032</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)                   │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ transformer_block_3                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">69</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">396,032</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)                   │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">69</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">450</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">58,050</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,708,610\u001b[0m (6.52 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,708,610</span> (6.52 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,708,610\u001b[0m (6.52 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,708,610</span> (6.52 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 138ms/step - accuracy: 0.1400 - loss: 4.6327 - val_accuracy: 0.2175 - val_loss: 3.6536 - learning_rate: 0.0010\n",
            "Epoch 2/5\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 63ms/step - accuracy: 0.2324 - loss: 3.5931 - val_accuracy: 0.3050 - val_loss: 3.2086 - learning_rate: 0.0010\n",
            "Epoch 3/5\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 62ms/step - accuracy: 0.2994 - loss: 3.2407 - val_accuracy: 0.3415 - val_loss: 3.0133 - learning_rate: 0.0010\n",
            "Epoch 4/5\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 65ms/step - accuracy: 0.3301 - loss: 3.0711 - val_accuracy: 0.3612 - val_loss: 2.9081 - learning_rate: 0.0010\n",
            "Epoch 5/5\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 66ms/step - accuracy: 0.3493 - loss: 2.9612 - val_accuracy: 0.3741 - val_loss: 2.8388 - learning_rate: 0.0010\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Génération de Texte et Évaluation BLEU\n",
        "\n",
        " Nous définissons plusieurs méthodes de génération (sampling, beam search et classique) et évaluons leurs performances à l'aide du score BLEU."
      ],
      "metadata": {
        "id": "BroDSIgCWUGG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Fonctions de génération"
      ],
      "metadata": {
        "id": "HD14WaCFWbMs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# %%\n",
        "def generate_text_with_sampling(model, sp, start_text, max_tokens=30, temperature=0.8,\n",
        "                                top_k=40, top_p=0.9, repetition_penalty=1.1, max_input_len=69):\n",
        "    eos_id = sp.piece_to_id(\"[EOS]\")\n",
        "    pad_id = sp.piece_to_id(\"[PAD]\")\n",
        "    bos_id = sp.piece_to_id(\"[BOS]\")\n",
        "    start_tokens = sp.encode_as_ids(start_text)\n",
        "    tokens = [bos_id] + start_tokens\n",
        "    for _ in range(max_tokens):\n",
        "        input_tokens = tokens[-max_input_len:]\n",
        "        if len(input_tokens) < max_input_len:\n",
        "            input_tokens = [pad_id] * (max_input_len - len(input_tokens)) + input_tokens\n",
        "        input_array = np.array([input_tokens], dtype=np.int32)\n",
        "        preds = model.predict(input_array, verbose=0)\n",
        "        logits = preds[0, -1, :]\n",
        "        for token_id in set(tokens):\n",
        "            if logits[token_id] < 0:\n",
        "                logits[token_id] *= repetition_penalty\n",
        "            else:\n",
        "                logits[token_id] /= repetition_penalty\n",
        "        logits = logits / temperature\n",
        "        probs = np.exp(logits) / np.sum(np.exp(logits))\n",
        "        sorted_indices = np.argsort(probs)[::-1]\n",
        "        sorted_probs = probs[sorted_indices]\n",
        "        if top_k > 0:\n",
        "            sorted_indices = sorted_indices[:top_k]\n",
        "            sorted_probs = sorted_probs[:top_k]\n",
        "        if top_p < 1.0:\n",
        "            cumulative_probs = np.cumsum(sorted_probs)\n",
        "            cutoff_index = np.searchsorted(cumulative_probs, top_p) + 1\n",
        "            sorted_indices = sorted_indices[:cutoff_index]\n",
        "            sorted_probs = sorted_probs[:cutoff_index]\n",
        "        sorted_probs = sorted_probs / np.sum(sorted_probs)\n",
        "        next_id = np.random.choice(sorted_indices, p=sorted_probs)\n",
        "        tokens.append(next_id)\n",
        "        if next_id == eos_id:\n",
        "            break\n",
        "    if eos_id in tokens:\n",
        "        eos_index = tokens.index(eos_id)\n",
        "        final_tokens = tokens[1:eos_index]\n",
        "    else:\n",
        "        final_tokens = tokens[1:]\n",
        "    final_tokens = [int(token) for token in final_tokens]\n",
        "    return sp.decode_ids(final_tokens)\n",
        "\n",
        "def beam_search_generate(model, sp, start_text, beam_width=3, max_tokens=30, temperature=0.8,\n",
        "                         length_penalty=0.8, repetition_penalty=1.1, max_input_len=69):\n",
        "    eos_id = sp.piece_to_id(\"[EOS]\")\n",
        "    pad_id = sp.piece_to_id(\"[PAD]\")\n",
        "    bos_id = sp.piece_to_id(\"[BOS]\")\n",
        "    prompt_tokens = sp.encode_as_ids(start_text)\n",
        "    initial_tokens = [bos_id] + prompt_tokens\n",
        "    beams = [(initial_tokens, 0.0)]\n",
        "    for _ in range(max_tokens):\n",
        "        new_beams = []\n",
        "        for seq, score in beams:\n",
        "            if seq[-1] == eos_id:\n",
        "                new_beams.append((seq, score))\n",
        "                continue\n",
        "            input_tokens = seq[-max_input_len:]\n",
        "            if len(input_tokens) < max_input_len:\n",
        "                input_tokens = [pad_id] * (max_input_len - len(input_tokens)) + input_tokens\n",
        "            input_array = np.array([input_tokens], dtype=np.int32)\n",
        "            preds = model.predict(input_array, verbose=0)\n",
        "            logits = preds[0, -1, :] / temperature\n",
        "            for token_id in set(seq):\n",
        "                if logits[token_id] < 0:\n",
        "                    logits[token_id] *= repetition_penalty\n",
        "                else:\n",
        "                    logits[token_id] /= repetition_penalty\n",
        "            probs = np.exp(logits) / np.sum(np.exp(logits))\n",
        "            top_indices = np.argsort(probs)[-beam_width:]\n",
        "            for token_id in top_indices:\n",
        "                token_prob = probs[token_id]\n",
        "                new_seq = seq + [token_id]\n",
        "                new_score = score + np.log(token_prob) / (len(new_seq) ** length_penalty)\n",
        "                new_beams.append((new_seq, new_score))\n",
        "        new_beams = sorted(new_beams, key=lambda x: x[1], reverse=True)\n",
        "        beams = new_beams[:beam_width]\n",
        "        if all(seq[-1] == eos_id for seq, _ in beams):\n",
        "            break\n",
        "    best_seq, best_score = beams[0]\n",
        "    if eos_id in best_seq:\n",
        "        eos_index = best_seq.index(eos_id)\n",
        "        best_seq = best_seq[1:eos_index]\n",
        "    else:\n",
        "        best_seq = best_seq[1:]\n",
        "    best_seq = [int(token) for token in best_seq]\n",
        "    return sp.decode_ids(best_seq)\n",
        "\n",
        "def generate_text_classic(model, sp, start_text, max_tokens=30, temperature=0.8, max_input_len=69):\n",
        "    eos_id = sp.piece_to_id(\"[EOS]\")\n",
        "    pad_id = sp.piece_to_id(\"[PAD]\")\n",
        "    bos_id = sp.piece_to_id(\"[BOS]\")\n",
        "    start_tokens = sp.encode_as_ids(start_text)\n",
        "    tokens = [bos_id] + start_tokens\n",
        "    for _ in range(max_tokens):\n",
        "        input_tokens = tokens[-max_input_len:]\n",
        "        if len(input_tokens) < max_input_len:\n",
        "            input_tokens = [pad_id] * (max_input_len - len(input_tokens)) + input_tokens\n",
        "        input_array = np.array([input_tokens], dtype=np.int32)\n",
        "        preds = model.predict(input_array, verbose=0)\n",
        "        logits = preds[0, -1, :]\n",
        "        logits = logits / temperature\n",
        "        probs = np.exp(logits) / np.sum(np.exp(logits))\n",
        "        next_id = np.random.choice(len(probs), p=probs)\n",
        "        tokens.append(next_id)\n",
        "        if next_id == eos_id:\n",
        "            break\n",
        "    if eos_id in tokens:\n",
        "        eos_index = tokens.index(eos_id)\n",
        "        final_tokens = tokens[1:eos_index]\n",
        "    else:\n",
        "        final_tokens = tokens[1:]\n",
        "    final_tokens = [int(token) for token in final_tokens]\n",
        "    return sp.decode_ids(final_tokens)"
      ],
      "metadata": {
        "id": "A4sautTFWkU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BLEU Evaluation\n",
        "\n",
        " Nous utilisons NLTK pour calculer le score BLEU entre une phrase de référence et la phrase générée."
      ],
      "metadata": {
        "id": "P2ZvKaUKWmvg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "def compute_bleu(reference, candidate):\n",
        "    reference_tokens = reference.split()\n",
        "    candidate_tokens = candidate.split()\n",
        "    smoothie = SmoothingFunction().method1\n",
        "    return sentence_bleu([reference_tokens], candidate_tokens, smoothing_function=smoothie)"
      ],
      "metadata": {
        "id": "DnmkASEEW1ED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chargement du Modèle et du Tokenizer\n",
        "\n",
        "On charge le modèle sauvegardé ainsi que le tokenizer SentencePiece."
      ],
      "metadata": {
        "id": "KaM8t9t6W2Qu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Génération de Texte et Évaluation BLEU\n",
        "\n",
        "Nous définissons quelques prompts de test et leurs références, puis nous générons du texte par les différentes méthodes."
      ],
      "metadata": {
        "id": "PF3sdgWWXM6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# %%\n",
        "test_prompts = [\n",
        "    \"I will make America\",\n",
        "    \"Fake news is\",\n",
        "    \"China is\",\n",
        "    \"Thank you\"\n",
        "]\n",
        "\n",
        "references = {\n",
        "    \"I will make America\": \"I will make America great again with strong leadership and policies.\",\n",
        "    \"Fake news is\": \"Fake news is misleading information used to manipulate public opinion.\",\n",
        "    \"China is\": \"China is emerging as a dominant force in the global economy.\",\n",
        "    \"Thank you\": \"Thank you for your support and trust in our vision.\"\n",
        "}\n",
        "\n",
        "# %%\n",
        "print(\"=== Sampling Generation ===\")\n",
        "for prompt in test_prompts:\n",
        "    gen_text = generate_text_with_sampling(\n",
        "        model, sp, prompt, max_tokens=30,\n",
        "        temperature=0.8, top_k=40, top_p=0.9,\n",
        "        repetition_penalty=1.1, max_input_len=69\n",
        "    )\n",
        "    print(f\"\\nPrompt: {prompt}\\nGenerated: {gen_text}\")\n",
        "\n",
        "print(\"\\n=== Beam Search Generation ===\")\n",
        "for prompt in test_prompts:\n",
        "    gen_text = beam_search_generate(\n",
        "        model, sp, prompt, beam_width=3, max_tokens=30,\n",
        "        temperature=0.8, length_penalty=0.8,\n",
        "        repetition_penalty=1.1, max_input_len=69\n",
        "    )\n",
        "    print(f\"\\nPrompt: {prompt}\\nGenerated: {gen_text}\")\n",
        "\n",
        "print(\"\\n=== Classic Temperature Generation ===\")\n",
        "for prompt in test_prompts:\n",
        "    gen_text = generate_text_classic(\n",
        "        model, sp, prompt, max_tokens=30,\n",
        "        temperature=0.8, max_input_len=69\n",
        "    )\n",
        "    print(f\"\\nPrompt: {prompt}\\nGenerated: {gen_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ECDK7rWXYlr",
        "outputId": "a3c9d7d7-d592-449b-e6d5-710912807681"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Sampling Generation ===\n",
            "\n",
            "Prompt: I will make America\n",
            "Generated: I will make AmericaTrump! Congace I late she few the Elub Top and oW endrujection sk\n",
            "\n",
            "Prompt: Fake news is\n",
            "Generated: Fake news is all oftol, with @Moredaison .......done to legar it and lass of he will ne\n",
            "\n",
            "Prompt: China is\n",
            "Generated: China is goaling about America and rigle feinutm as. Thanks Fast Alore Is the hunm\n",
            "\n",
            "Prompt: Thank you\n",
            "Generated: Thank you Boline Smit! (TrumpDatton by all calise lanargin and we far sidat\n",
            "\n",
            "=== Beam Search Generation ===\n",
            "\n",
            "Prompt: I will make America\n",
            "Generated: I will make America Great! \n",
            "\n",
            "Prompt: Fake news is\n",
            "Generated: Fake news is a lot official! \n",
            "\n",
            "Prompt: China is\n",
            "Generated: China is a little energy. \n",
            "\n",
            "Prompt: Thank you\n",
            "Generated: Thank you! \n",
            "\n",
            "=== Classic Temperature Generation ===\n",
            "\n",
            "Prompt: I will make America\n",
            "Generated: I will make America ...oc myepprent st ...ivethare theyountthergmericck...ratit exe Wh re need Mrat thisle runce\n",
            "\n",
            "Prompt: Fake news is\n",
            "Generated: Fake news is should our Thank gootR U dotsAT all)cerealuresses jobvenigVlyiow St arerent\n",
            "\n",
            "Prompt: China is\n",
            "Generated: China isredilagn yourlicobteiteriountRostinaanationF con k stoo Hordontraing Wh StoutDonaldTrump\n",
            "\n",
            "Prompt: Thank you\n",
            "Generated: Thank youer, been3 haveocrat has Thank o worki Aasiceansote seist not job shouldoo Jud h his unicandews\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BLEU Score Evaluation\n",
        "\n",
        "Nous évaluons les générations par les trois méthodes en calculant leur score BLEU par rapport aux références.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Z0HsGUTQXfwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# %%\n",
        "print(\"\\n=== BLEU Score Evaluation (Classic Generation) ===\")\n",
        "for prompt in test_prompts:\n",
        "    generated_text = generate_text_classic(\n",
        "        model, sp, prompt, max_tokens=30,\n",
        "        temperature=0.8, max_input_len=69\n",
        "    )\n",
        "    if prompt in references:\n",
        "        bleu = compute_bleu(references[prompt], generated_text)\n",
        "        print(f\"\\nPrompt: {prompt}\\nGenerated: {generated_text}\\nBLEU Score: {bleu:.4f}\")\n",
        "\n",
        "# %%\n",
        "print(\"\\n=== BLEU Score Evaluation (Sampling Generation) ===\")\n",
        "for prompt in test_prompts:\n",
        "    generated_text = generate_text_with_sampling(\n",
        "        model, sp, prompt, max_tokens=30,\n",
        "        temperature=0.8, top_k=40, top_p=0.9,\n",
        "        repetition_penalty=1.1, max_input_len=69\n",
        "    )\n",
        "    if prompt in references:\n",
        "        bleu = compute_bleu(references[prompt], generated_text)\n",
        "        print(f\"\\nPrompt: {prompt}\\nGenerated: {generated_text}\\nBLEU Score: {bleu:.4f}\")\n",
        "\n",
        "# %%\n",
        "print(\"\\n=== BLEU Score Evaluation (Beam Search Generation) ===\")\n",
        "for prompt in test_prompts:\n",
        "    generated_text = beam_search_generate(\n",
        "        model, sp, prompt, beam_width=3, max_tokens=30,\n",
        "        temperature=0.8, length_penalty=0.8,\n",
        "        repetition_penalty=1.1, max_input_len=69\n",
        "    )\n",
        "    if prompt in references:\n",
        "        bleu = compute_bleu(references[prompt], generated_text)\n",
        "        print(f\"\\nPrompt: {prompt}\\nGenerated: {generated_text}\\nBLEU Score: {bleu:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tR6zzJsqXjiE",
        "outputId": "77166dcb-0b1e-4b74-d64d-54331b82944a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== BLEU Score Evaluation (Classic Generation) ===\n",
            "\n",
            "Prompt: I will make America\n",
            "Generated: I will make America su6 ( aselehenmp Ent Thanks V00 f whatonald Th moreobpent people[BOS]usting am year le5ousld\n",
            "BLEU Score: 0.1345\n",
            "\n",
            "Prompt: Fake news is\n",
            "Generated: Fake news isontoll it yj Trump[BOS] time\n",
            "BLEU Score: 0.0455\n",
            "\n",
            "Prompt: China is\n",
            "Generated: China is The V0 w B by de[uelayV job mO thate F want(20te Trump al H2016 big coniveia\n",
            "BLEU Score: 0.0228\n",
            "\n",
            "Prompt: Thank you\n",
            "Generated: Thank you re prl d St amagusW theT greatbamaIavctteraderealove Jass thatadeulade weAurut\n",
            "BLEU Score: 0.0360\n",
            "\n",
            "=== BLEU Score Evaluation (Sampling Generation) ===\n",
            "\n",
            "Prompt: I will make America\n",
            "Generated: I will make America Trump......Yelain tunt ratis, this seatem out of fieilen is lybolller\n",
            "BLEU Score: 0.1778\n",
            "\n",
            "Prompt: Fake news is\n",
            "Generated: Fake news is now moneL! Promite? Whate 3 would thId. Great suffeners, dional Petm\n",
            "BLEU Score: 0.0707\n",
            "\n",
            "Prompt: China is\n",
            "Generated: China is now it so treridce. OAMAMERO! 4I. Busbad striend toug\n",
            "BLEU Score: 0.0399\n",
            "\n",
            "Prompt: Thank you\n",
            "Generated: Thank you 4897? Thop act for his serecterment w me bed for expope in lom\n",
            "BLEU Score: 0.0332\n",
            "\n",
            "=== BLEU Score Evaluation (Beam Search Generation) ===\n",
            "\n",
            "Prompt: I will make America\n",
            "Generated: I will make America Great! \n",
            "BLEU Score: 0.2014\n",
            "\n",
            "Prompt: Fake news is\n",
            "Generated: Fake news is a lot official! \n",
            "BLEU Score: 0.1037\n",
            "\n",
            "Prompt: China is\n",
            "Generated: China is a little energy. \n",
            "BLEU Score: 0.0379\n",
            "\n",
            "Prompt: Thank you\n",
            "Generated: Thank you! \n",
            "BLEU Score: 0.0027\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}